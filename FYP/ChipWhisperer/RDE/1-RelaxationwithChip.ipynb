{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95701fed-5230-4b02-80af-7195c5667169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import importlib\n",
    "\n",
    "# Seed reset function\n",
    "def reset_random_seeds(seed_value=42):\n",
    "    python_random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "\n",
    "reset_random_seeds()\n",
    "\n",
    "# AES S-box and utility functions\n",
    "AES_Sbox = [\n",
    "    0x63, 0x7c, 0x77, 0x7b, 0xf2, 0x6b, 0x6f, 0xc5, 0x30, 0x01, 0x67, 0x2b, 0xfe, 0xd7, 0xab, 0x76,\n",
    "    0xca, 0x82, 0xc9, 0x7d, 0xfa, 0x59, 0x47, 0xf0, 0xad, 0xd4, 0xa2, 0xaf, 0x9c, 0xa4, 0x72, 0xc0,\n",
    "    0xb7, 0xfd, 0x93, 0x26, 0x36, 0x3f, 0xf7, 0xcc, 0x34, 0xa5, 0xe5, 0xf1, 0x71, 0xd8, 0x31, 0x15,\n",
    "    0x04, 0xc7, 0x23, 0xc3, 0x18, 0x96, 0x05, 0x9a, 0x07, 0x12, 0x80, 0xe2, 0xeb, 0x27, 0xb2, 0x75,\n",
    "    0x09, 0x83, 0x2c, 0x1a, 0x1b, 0x6e, 0x5a, 0xa0, 0x52, 0x3b, 0xd6, 0xb3, 0x29, 0xe3, 0x2f, 0x84,\n",
    "    0x53, 0xd1, 0x00, 0xed, 0x20, 0xfc, 0xb1, 0x5b, 0x6a, 0xcb, 0xbe, 0x39, 0x4a, 0x4c, 0x58, 0xcf,\n",
    "    0xd0, 0xef, 0xaa, 0xfb, 0x43, 0x4d, 0x33, 0x85, 0x45, 0xf9, 0x02, 0x7f, 0x50, 0x3c, 0x9f, 0xa8,\n",
    "    0x51, 0xa3, 0x40, 0x8f, 0x92, 0x9d, 0x38, 0xf5, 0xbc, 0xb6, 0xda, 0x21, 0x10, 0xff, 0xf3, 0xd2,\n",
    "    0xcd, 0x0c, 0x13, 0xec, 0x5f, 0x97, 0x44, 0x17, 0xc4, 0xa7, 0x7e, 0x3d, 0x64, 0x5d, 0x19, 0x73,\n",
    "    0x60, 0x81, 0x4f, 0xdc, 0x22, 0x2a, 0x90, 0x88, 0x46, 0xee, 0xb8, 0x14, 0xde, 0x5e, 0x0b, 0xdb,\n",
    "    0xe0, 0x32, 0x3a, 0x0a, 0x49, 0x06, 0x24, 0x5c, 0xc2, 0xd3, 0xac, 0x62, 0x91, 0x95, 0xe4, 0x79,\n",
    "    0xe7, 0xc8, 0x37, 0x6d, 0x8d, 0xd5, 0x4e, 0xa9, 0x6c, 0x56, 0xf4, 0xea, 0x65, 0x7a, 0xae, 0x08,\n",
    "    0xba, 0x78, 0x25, 0x2e, 0x1c, 0xa6, 0xb4, 0xc6, 0xe8, 0xdd, 0x74, 0x1f, 0x4b, 0xbd, 0x8b, 0x8a,\n",
    "    0x70, 0x3e, 0xb5, 0x66, 0x48, 0x03, 0xf6, 0x0e, 0x61, 0x35, 0x57, 0xb9, 0x86, 0xc1, 0x1d, 0x9e,\n",
    "    0xe1, 0xf8, 0x98, 0x11, 0x69, 0xd9, 0x8e, 0x94, 0x9b, 0x1e, 0x87, 0xe9, 0xce, 0x55, 0x28, 0xdf,\n",
    "    0x8c, 0xa1, 0x89, 0x0d, 0xbf, 0xe6, 0x42, 0x68, 0x41, 0x99, 0x2d, 0x0f, 0xb0, 0x54, 0xbb, 0x16\n",
    "]\n",
    "\n",
    "# Function to compute expected distortion\n",
    "def compute_expected_distortion(x, s, Vs, model):\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    s = tf.cast(s, tf.float32)\n",
    "    Vs = tf.cast(Vs, tf.float32)\n",
    "    perturbed_x = x * s + (1 - s) * Vs\n",
    "    perturbed_x = tf.reshape(perturbed_x, (1, -1, 1))\n",
    "    x_reshaped = tf.reshape(x, (1, -1, 1))\n",
    "    original_pred = model.predict(x_reshaped)\n",
    "    perturbed_pred = model.predict(perturbed_x)\n",
    "    return np.linalg.norm(original_pred - perturbed_pred)\n",
    "\n",
    "# 1-Relaxation with Lagrange Multiplier\n",
    "def lagrange_multiplier_optimization(theta, lamb, lr, iterations, batch_size, model, x_train, Vs):\n",
    "    for _ in range(iterations):\n",
    "        with tf.GradientTape() as tape:\n",
    "            s = tf.sigmoid(theta)\n",
    "            batch_loss = 0\n",
    "            for i in range(batch_size):\n",
    "                x_i_reshaped = tf.reshape(x_train[i], (trace_length, 1))\n",
    "                Vs_i_reshaped = tf.reshape(Vs[i], (trace_length, 1))\n",
    "                s_reshaped = tf.reshape(s, (trace_length, 1))\n",
    "                batch_loss += compute_expected_distortion(x_i_reshaped, s_reshaped, Vs_i_reshaped, model)\n",
    "            loss = batch_loss / batch_size + lamb * tf.reduce_sum(s)\n",
    "        gradients = tape.gradient(loss, [theta])\n",
    "        theta.assign_sub(lr * gradients[0])\n",
    "\n",
    "# Hyperparameters for CNN\n",
    "def get_hyperparameters_cnn(regularization=False):\n",
    "    hyperparameters = {}\n",
    "    hyperparameters_mlp = get_hyperparameters_mlp(regularization=regularization, max_dense_layers=3)\n",
    "    for key, value in hyperparameters_mlp.items():\n",
    "        hyperparameters[key] = value\n",
    "\n",
    "    conv_layers = random.choice([1, 2, 3, 4])\n",
    "    kernels = []\n",
    "    strides = []\n",
    "    filters = []\n",
    "    pooling_types = []\n",
    "    pooling_sizes = []\n",
    "    pooling_strides = []\n",
    "    pooling_type = random.choice([\"Average\", \"Max\"])\n",
    "\n",
    "    for conv_layer in range(1, conv_layers + 1):\n",
    "        kernel = random.randrange(2, 10, 1)\n",
    "        kernels.append(kernel)\n",
    "        strides.append(int(kernel / 2))\n",
    "        if conv_layer == 1:\n",
    "            filters.append(random.choice([u for u in range(8, 65, 8)]))\n",
    "        else:\n",
    "            filters.append(filters[conv_layer - 2] * 2)\n",
    "        pool_size = random.choice([2, 3, 4, 5])\n",
    "        pooling_sizes.append(pool_size)\n",
    "        pooling_strides.append(pool_size)\n",
    "        pooling_types.append(pooling_type)\n",
    "\n",
    "    hyperparameters[\"conv_layers\"] = conv_layers\n",
    "    hyperparameters[\"kernels\"] = kernels\n",
    "    hyperparameters[\"strides\"] = strides\n",
    "    hyperparameters[\"filters\"] = filters\n",
    "    hyperparameters[\"pooling_sizes\"] = pooling_sizes\n",
    "    hyperparameters[\"pooling_strides\"] = pooling_strides\n",
    "    hyperparameters[\"pooling_types\"] = pooling_types\n",
    "\n",
    "    return hyperparameters\n",
    "\n",
    "def get_optimizer(optimizer, learning_rate):\n",
    "    module_name = importlib.import_module(\"tensorflow.keras.optimizers\")\n",
    "    optimizer_class = getattr(module_name, optimizer)\n",
    "    return optimizer_class(learning_rate=learning_rate)\n",
    "\n",
    "def mlp_random(classes, number_of_samples, regularization=False, hp=None):\n",
    "    hp = get_hyperparameters_mlp(regularization=regularization) if hp is None else hp\n",
    "    tf_random_seed = np.random.randint(1048576)\n",
    "    tf.random.set_seed(tf_random_seed)\n",
    "\n",
    "    inputs = Input(shape=number_of_samples)\n",
    "    x = None\n",
    "    for layer_index in range(hp[\"layers\"]):\n",
    "        if regularization and hp[\"regularization\"] != \"dropout\":\n",
    "            x = Dense(hp[\"neurons\"], activation=hp[\"activation\"], kernel_regularizer=get_reg(hp),\n",
    "                      kernel_initializer=hp[\"kernel_initializer\"],\n",
    "                      name='dense_{}'.format(layer_index))(inputs if layer_index == 0 else x)\n",
    "        else:\n",
    "            x = Dense(hp[\"neurons\"], activation=hp[\"activation\"], kernel_initializer=hp[\"kernel_initializer\"],\n",
    "                      name='dense_{}'.format(layer_index))(inputs if layer_index == 0 else x)\n",
    "        if regularization and hp[\"regularization\"] == \"dropout\":\n",
    "            x = Dropout(get_reg(hp))(x)\n",
    "\n",
    "    outputs = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "    model = Model(inputs, outputs, name='random_mlp')\n",
    "    optimizer = get_optimizer(hp[\"optimizer\"], hp[\"learning_rate\"])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model, tf_random_seed, hp\n",
    "\n",
    "def cnn_random(classes, number_of_samples, regularization=False, hp=None):\n",
    "    hp = get_hyperparameters_cnn(regularization=regularization) if hp is None else hp\n",
    "    tf_random_seed = np.random.randint(1048576)\n",
    "    tf.random.set_seed(tf_random_seed)\n",
    "\n",
    "    inputs = Input(shape=(number_of_samples, 1))\n",
    "    x = None\n",
    "    for layer_index in range(hp[\"conv_layers\"]):\n",
    "        x = Conv1D(kernel_size=hp[\"kernels\"][layer_index], strides=hp[\"strides\"][layer_index], filters=hp[\"filters\"][layer_index],\n",
    "                   activation=hp[\"activation\"], padding=\"same\")(inputs if layer_index == 0 else x)\n",
    "        if hp[\"pooling_types\"][layer_index] == \"Average\":\n",
    "            x = AveragePooling1D(pool_size=hp[\"pooling_sizes\"][layer_index], strides=hp[\"pooling_strides\"][layer_index], padding=\"same\")(x)\n",
    "        else:\n",
    "            x = MaxPooling1D(pool_size=hp[\"pooling_sizes\"][layer_index], strides=hp[\"pooling_strides\"][layer_index], padding=\"same\")(x)\n",
    "        x = BatchNormalization()(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    for layer_index in range(hp[\"layers\"]):\n",
    "        if regularization and hp[\"regularization\"] != \"dropout\":\n",
    "            x = Dense(hp[\"neurons\"], activation=hp[\"activation\"], kernel_regularizer=get_reg(hp),\n",
    "                      kernel_initializer=hp[\"kernel_initializer\"], name='dense_{}'.format(layer_index))(x)\n",
    "        else:\n",
    "            x = Dense(hp[\"neurons\"], activation=hp[\"activation\"], kernel_initializer=hp[\"kernel_initializer\"],\n",
    "                      name='dense_{}'.format(layer_index))(x)\n",
    "        if regularization and hp[\"regularization\"] == \"dropout\":\n",
    "            x = Dropout(hp[\"dropout\"])(x)\n",
    "\n",
    "    outputs = Dense(classes, activation='softmax', name='predictions')(x)\n",
    "    model = Model(inputs, outputs, name='random_cnn')\n",
    "    optimizer = get_optimizer(hp[\"optimizer\"], hp[\"learning_rate\"])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model, tf_random_seed, hp\n",
    "\n",
    "# Function for Guessing Entropy (GE) and Correlation Power Analysis (CPA)\n",
    "def perform_attacks(nb_traces=1000, predictions=None, plt_attack=None, correct_key=None, leakage=\"HW\", dataset=None, nb_attacks=100, shuffle=True):\n",
    "    all_rk_evol = np.zeros((nb_attacks, nb_traces))\n",
    "    all_key_log_prob = np.zeros(256)\n",
    "    for i in tqdm(range(nb_attacks)):\n",
    "        if shuffle:\n",
    "            l = list(zip(predictions, plt_attack))\n",
    "            random.shuffle(l)\n",
    "            sp, splt = list(zip(*l))\n",
    "            sp = np.array(sp)\n",
    "            splt = np.array(splt)\n",
    "            att_pred = sp[:nb_traces]\n",
    "            att_plt = splt[:nb_traces]\n",
    "        else:\n",
    "            att_pred = predictions[:nb_traces]\n",
    "            att_plt = plt_attack[:nb_traces]\n",
    "        rank_evol, key_log_prob = rank_compute(att_pred, att_plt, correct_key, leakage=leakage, dataset=dataset)\n",
    "        all_rk_evol[i] = rank_evol\n",
    "        all_key_log_prob += key_log_prob\n",
    "    return np.mean(all_rk_evol, axis=0), key_log_prob\n",
    "\n",
    "def rank_compute(prediction, att_plt, correct_key, leakage, dataset):\n",
    "    hw = [bin(x).count(\"1\") for x in range(256)]\n",
    "    (nb_traces, nb_hyp) = prediction.shape\n",
    "    key_log_prob = np.zeros(256)\n",
    "    prediction = np.log(prediction + 1e-40)\n",
    "    rank_evol = np.full(nb_traces, 255)\n",
    "    for i in range(nb_traces):\n",
    "        for k in range(256):\n",
    "            att_byte = int(att_plt[i, 0])\n",
    "            if dataset == \"AES_HD_ext\":\n",
    "                if leakage == 'ID':\n",
    "                    key_log_prob[k] += prediction[i, AES_Sbox_inv[k ^ att_byte] ^ att_byte]\n",
    "                else:\n",
    "                    key_log_prob[k] += prediction[i, hw[AES_Sbox_inv[k ^ att_byte] ^ att_byte]]\n",
    "            elif dataset == \"AES_HD_ext_ID\":\n",
    "                if leakage == 'ID':\n",
    "                    key_log_prob[k] += prediction[i, AES_Sbox_inv[k ^ att_byte]]\n",
    "                else:\n",
    "                    key_log_prob[k] += prediction[i, hw[AES_Sbox_inv[k ^ att_byte]]]\n",
    "            else:\n",
    "                if leakage == 'ID':\n",
    "                    key_log_prob[k] += prediction[i, AES_Sbox[k ^ att_byte]]\n",
    "                else:\n",
    "                    key_log_prob[k] += prediction[i, hw[AES_Sbox[k ^ att_byte]]]\n",
    "        rank_evol[i] = rk_key(key_log_prob, correct_key)\n",
    "    return rank_evol, key_log_prob\n",
    "\n",
    "def rk_key(rank_array, key):\n",
    "    key_val = rank_array[key]\n",
    "    final_rank = np.float32(np.where(np.sort(rank_array)[::-1] == key_val)[0][0])\n",
    "    if math.isnan(float(final_rank)) or math.isinf(float(final_rank)):\n",
    "        return np.float32(256)\n",
    "    else:\n",
    "        return np.float32(final_rank)\n",
    "\n",
    "def NTGE_fn(GE):\n",
    "    NTGE = float('inf')\n",
    "    for i in range(GE.shape[0] - 1, -1, -1):\n",
    "        if GE[i] > 0:\n",
    "            NTGE = i\n",
    "            break\n",
    "    return NTGE\n",
    "\n",
    "def aes_label_cpa(plaintexts, correct_key, leakage):\n",
    "    num_traces = plaintexts.shape[0]\n",
    "    labels_for_snr = np.zeros(num_traces)\n",
    "    for i in range(num_traces):\n",
    "        if leakage == 'HW':\n",
    "            labels_for_snr[i] = HW(AES_Sbox[plaintexts[i, 0] ^ correct_key])\n",
    "        elif leakage == 'ID':\n",
    "            labels_for_snr[i] = AES_Sbox[plaintexts[i, 0] ^ correct_key]\n",
    "    return labels_for_snr\n",
    "\n",
    "def perform_cpa_all_keys(traces, plaintexts, num_keys=256):\n",
    "    num_traces, num_samples = traces.shape\n",
    "    correlations = np.zeros((num_keys, num_samples))\n",
    "    for key_guess in tqdm(range(num_keys)):\n",
    "        labels_for_cpa = aes_label_cpa(plaintexts, key_guess, leakage)\n",
    "        for t in range(num_samples):\n",
    "            correlations[key_guess, t] = abs(np.corrcoef(labels_for_cpa[:num_traces], traces[:, t])[1, 0])\n",
    "    return correlations\n",
    "\n",
    "def plot_correlations(correlations, correct_key):\n",
    "    num_keys, num_samples = correlations.shape\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for key_guess in range(num_keys):\n",
    "        if key_guess != correct_key:\n",
    "            plt.plot(correlations[key_guess], color='grey', label='Other Keys' if 'Other Keys' not in plt.gca().get_legend_handles_labels()[1] else \"\", alpha=0.5, linewidth=1)\n",
    "    plt.plot(correlations[correct_key], color='blue', label=f'Correct Key: {correct_key:02X}', linewidth=2)\n",
    "    plt.title('Correlation Power Analysis Across All Key Guesses')\n",
    "    plt.xlabel('Sample Points')\n",
    "    plt.ylabel('Absolute Correlation')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Data loading and preprocessing\n",
    "    num_traces = 10000\n",
    "    trace_length = 5000\n",
    "    leakage = \"ID\"\n",
    "    chipwhisper_folder = '/home/localuserplr/Documents/FYP/chipwhisperer/'\n",
    "\n",
    "    X = np.load(chipwhisper_folder + 'traces.npy')[:num_traces]\n",
    "    y = np.load(chipwhisper_folder + 'labels.npy')[:num_traces]\n",
    "    plaintexts = np.load(chipwhisper_folder + 'plain.npy')[:num_traces]\n",
    "    keys = np.load(chipwhisper_folder + 'key.npy')[:num_traces]\n",
    "\n",
    "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "    X_train_val, X_attack, y_train_val, y_attack, plaintexts_train_val, plaintexts_attack = train_test_split(\n",
    "        X, y, plaintexts, test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n",
    "\n",
    "    y_train_categorical = tf.keras.utils.to_categorical(y_train, num_classes=256)\n",
    "    y_val_categorical = tf.keras.utils.to_categorical(y_val, num_classes=256)\n",
    "\n",
    "    classes = 256\n",
    "    number_of_samples = trace_length\n",
    "    model_type = \"cnn\"\n",
    "    regularization = True\n",
    "    \n",
    "    # Parameters for perturbation\n",
    "    k = trace_length\n",
    "    lamb = 0.1\n",
    "    num_iterations = 10\n",
    "    learning_rate = 0.01\n",
    "    batch_size = 32\n",
    "\n",
    "    # Initialize theta for 1-Relaxation with Lagrange Multiplier\n",
    "    theta = tf.Variable(np.ones(k) * 0.5, dtype=tf.float32)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.optimizers.Adam(learning_rate)\n",
    "\n",
    "    # Initialize model\n",
    "    model = Sequential([\n",
    "        Conv1D(32, kernel_size=3, activation='relu', input_shape=(trace_length, 1)),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Random data (example)\n",
    "    x = X_train[:batch_size]  # Example batch\n",
    "    Vs = np.random.randn(batch_size, k)\n",
    "\n",
    "    # Optimization loop for feature selection using 1-Relaxation with Lagrange Multiplier\n",
    "    lagrange_multiplier_optimization(theta, lamb, learning_rate, num_iterations, batch_size, model, x, Vs)\n",
    "\n",
    "    # Resulting mask\n",
    "    s = tf.sigmoid(theta).numpy()\n",
    "\n",
    "    # Apply the mask to the training data\n",
    "    X_train_masked = X_train * s[np.newaxis, :, np.newaxis]\n",
    "    X_val_masked = X_val * s[np.newaxis, :, np.newaxis]\n",
    "\n",
    "    # Function to create and train models\n",
    "    def create_and_train_model(x_train, y_train, x_val, y_val, model_type=\"cnn\", regularization=True):\n",
    "        classes = 256\n",
    "        number_of_samples = x_train.shape[1]\n",
    "        if model_type == \"mlp\":\n",
    "            hp = get_hyperparameters_mlp(regularization=regularization)\n",
    "            model, seed, hp = mlp_random(classes, number_of_samples, hp=hp, regularization=regularization)\n",
    "        else:\n",
    "            hp = get_hyperparameters_cnn(regularization=regularization)\n",
    "            model, seed, hp = cnn_random(classes, number_of_samples, hp=hp, regularization=regularization)\n",
    "\n",
    "        history = model.fit(x_train, y_train, epochs=50, validation_data=(x_val, y_val), batch_size=hp[\"batch_size\"])\n",
    "        return model, history\n",
    "\n",
    "    # Function to plot accuracy\n",
    "    def plot_accuracy(history, title):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Train and evaluate models with and without 1-Relaxation with Lagrange Multiplier\n",
    "    models = []\n",
    "    histories_with_lagrange = []\n",
    "    histories_without_lagrange = []\n",
    "\n",
    "    # With 1-Relaxation with Lagrange Multiplier feature selection\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            model, history = create_and_train_model(X_train_masked, y_train_categorical, X_val_masked, y_val_categorical, model_type=\"cnn\", regularization=True)\n",
    "            models.append(model)\n",
    "            histories_with_lagrange.append(history)\n",
    "\n",
    "            print(f\"Model {i+1} summary (with 1-Relaxation with Lagrange Multiplier):\")\n",
    "            model.summary()\n",
    "\n",
    "            predictions = model.predict(X_attack)\n",
    "            predicted_classes = np.argmax(predictions, axis=1)\n",
    "            accuracy = np.mean(predicted_classes == y_attack)\n",
    "            print(f\"Test set accuracy for model {i+1} (with 1-Relaxation with Lagrange Multiplier): {accuracy}\")\n",
    "\n",
    "            plot_accuracy(history, f'Model {i+1} Accuracy (with 1-Relaxation with Lagrange Multiplier)')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while creating model {i+1} (with 1-Relaxation with Lagrange Multiplier): {e}\")\n",
    "\n",
    "    # Without 1-Relaxation with Lagrange Multiplier feature selection\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            model, history = create_and_train_model(X_train, y_train_categorical, X_val, y_val_categorical, model_type=\"cnn\", regularization=True)\n",
    "            models.append(model)\n",
    "            histories_without_lagrange.append(history)\n",
    "\n",
    "            print(f\"Model {i+1} summary (without 1-Relaxation with Lagrange Multiplier):\")\n",
    "            model.summary()\n",
    "\n",
    "            predictions = model.predict(X_attack)\n",
    "            predicted_classes = np.argmax(predictions, axis=1)\n",
    "            accuracy = np.mean(predicted_classes == y_attack)\n",
    "            print(f\"Test set accuracy for model {i+1} (without 1-Relaxation with Lagrange Multiplier): {accuracy}\")\n",
    "\n",
    "            plot_accuracy(history, f'Model {i+1} Accuracy (without 1-Relaxation with Lagrange Multiplier)')\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while creating model {i+1} (without 1-Relaxation with Lagrange Multiplier): {e}\")\n",
    "\n",
    "    # Compare the average accuracy of models with and without 1-Relaxation with Lagrange Multiplier feature selection\n",
    "    if histories_with_lagrange:\n",
    "        avg_train_acc_with_lagrange = np.mean([history.history['accuracy'][-1] for history in histories_with_lagrange])\n",
    "        avg_val_acc_with_lagrange = np.mean([history.history['val_accuracy'][-1] for history in histories_with_lagrange])\n",
    "    else:\n",
    "        avg_train_acc_with_lagrange = np.nan\n",
    "        avg_val_acc_with_lagrange = np.nan\n",
    "\n",
    "    if histories_without_lagrange:\n",
    "        avg_train_acc_without_lagrange = np.mean([history.history['accuracy'][-1] for history in histories_without_lagrange])\n",
    "        avg_val_acc_without_lagrange = np.mean([history.history['val_accuracy'][-1] for history in histories_without_lagrange])\n",
    "    else:\n",
    "        avg_train_acc_without_lagrange = np.nan\n",
    "        avg_val_acc_without_lagrange = np.nan\n",
    "\n",
    "    print(f\"Average training accuracy with 1-Relaxation with Lagrange Multiplier: {avg_train_acc_with_lagrange}\")\n",
    "    print(f\"Average validation accuracy with 1-Relaxation with Lagrange Multiplier: {avg_val_acc_with_lagrange}\")\n",
    "    print(f\"Average training accuracy without 1-Relaxation with Lagrange Multiplier: {avg_train_acc_without_lagrange}\")\n",
    "    print(f\"Average validation accuracy without 1-Relaxation with Lagrange Multiplier: {avg_val_acc_without_lagrange}\")\n",
    "\n",
    "    # Plot comparison of average accuracy\n",
    "    labels = ['Training Accuracy', 'Validation Accuracy']\n",
    "    with_lagrange = [avg_train_acc_with_lagrange, avg_val_acc_with_lagrange]\n",
    "    without_lagrange = [avg_train_acc_without_lagrange, avg_val_acc_without_lagrange]\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    rects1 = ax.bar(x - width/2, with_lagrange, width, label='With 1-Relaxation with Lagrange Multiplier')\n",
    "    rects2 = ax.bar(x + width/2, without_lagrange, width, label='Without 1-Relaxation with Lagrange Multiplier')\n",
    "\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Average Accuracy with and without 1-Relaxation with Lagrange Multiplier Feature Selection')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
